{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/richard/.pyenv/versions/3.12.2/envs/research/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import clip\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from typing import Union\n",
    "import torch.nn.functional as F\n",
    "from clip.model import CLIP, ModifiedResNet, AttentionPool2d\n",
    "\n",
    "from models.part_cem import PartCEM\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m part_cem \u001b[38;5;241m=\u001b[39m \u001b[43mPartCEM\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Work/part-concept/models/part_cem.py:9\u001b[0m, in \u001b[0;36mPartCEM.__init__\u001b[0;34m(self, backbone, num_concepts, num_classes)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, backbone\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresnet50\u001b[39m\u001b[38;5;124m'\u001b[39m, num_concepts\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m112\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mPartCEM\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m()\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m=\u001b[39m num_concepts\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone \u001b[38;5;241m=\u001b[39m timm\u001b[38;5;241m.\u001b[39mcreate_model(backbone, pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "part_cem = PartCEM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([8, 113, 7, 7]), torch.Size([8, 112]), torch.Size([8, 200]))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(8, 3, 224, 224)\n",
    "cpt_score_maps, cpt_logits, label_logits = part_cem(x)\n",
    "cpt_score_maps.shape, cpt_logits.shape, label_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_model, img_transforms = clip.load('RN50', device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPModified(nn.Module):\n",
    "    def __init__(self, model: CLIP):\n",
    "        super(CLIPModified, self).__init__()\n",
    "        assert type(model.visual) == ModifiedResNet\n",
    "        assert type(model.visual.attnpool) == AttentionPool2d\n",
    "\n",
    "        image_model = copy.deepcopy(model.visual)\n",
    "        image_model.attnpool = nn.Identity()\n",
    "        self.model = image_model\n",
    "        self.attnpool = copy.deepcopy(model.visual.attnpool)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        f = self.model(x) # raw features without attn_pool\n",
    "        b, c, h, w = f.shape  # [BS, 2048, 7, 7]\n",
    "        g = self.attnpool(f)  # [BS, 1024]\n",
    "        x = f.permute(0, 2, 3, 1)  # [BS, 7, 7, 2048]\n",
    "        x = x.reshape(-1, c)  # [BS*7*7, 2048]\n",
    "        x = x[..., None, None]  # [BS*7*7, 2048, 1, 1]\n",
    "        x = x.expand(-1, -1, 7, 7)  # [BS*7*7, 2048, 7, 7]\n",
    "        x = self.attnpool(x)  # [BS*7*7, 1024]\n",
    "        x = x.reshape(b, h, w, -1)  # [BS, 7, 7, 1024]\n",
    "        x = x.permute(0, 3, 1, 2)  # [BS, 1024, 7, 7]\n",
    "        return f, g, x\n",
    "\n",
    "clip_modified = CLIPModified(clip_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(8, 3, 224, 224)\n",
    "f, g, x = clip_modified(x)\n",
    "f.shape, g.shape, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = 'datasets/CUB/CUB_200_2011/images/083.White_breasted_Kingfisher/White_Breasted_Kingfisher_0035_73290.jpg'\n",
    "texts = ['brown bird head', 'black bird head', 'white bird head', 'background']\n",
    "texts_tokens = clip.tokenize(texts=texts)\n",
    "img = Image.open(img_dir).convert('RGB')\n",
    "with torch.no_grad():\n",
    "    img = img_transforms(img=img)\n",
    "    features = clip_modified(img.unsqueeze(0))\n",
    "    texts_encoded = clip_model.encode_text(texts_tokens)\n",
    "    f, g, x = features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, texts_encoded = x.squeeze(0), texts_encoded.squeeze(0)\n",
    "x = x / x.norm(dim=0, keepdim=True)\n",
    "texts_encoded = texts_encoded / texts_encoded.norm()\n",
    "score_map = F.softmax(torch.einsum('kd,dwh->kwh', texts_encoded, x), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_map[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(score_map[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.open(img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(torch.tensor([-1., -1., 5.0, 5.0]), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = torch.randn(32, 100)[..., None]\n",
    "concepts = torch.rand(100, 1024)[None, ...].expand(32, -1, -1)\n",
    "(scores * concepts).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = concepts.unbind(dim=1)\n",
    "f = torch.cat(ts, dim=-1)\n",
    "len(ts), ts[0].shape, f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "102400*100/1000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
