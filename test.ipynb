{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIPSeg Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'clipseg.configuration_clipseg.CLIPSegConfig'> 512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of CLIPSegForImageSegmentation were not initialized from the model checkpoint at CIDAS/clipseg-rd64-refined and are newly initialized: ['non_object_embedding', 'text_adapter.fc.0.weight', 'text_adapter.fc.2.weight', 'tunable_linear.weight', 'visual_adapter.fc.0.weight', 'visual_adapter.fc.2.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from clipseg_model import CLIPSeg\n",
    "from clipseg_train import load_concepts\n",
    "\n",
    "with open('concepts/CUB/parts.txt') as fp:\n",
    "    part_texts = fp.read().splitlines()\n",
    "\n",
    "concept_dict = load_concepts()\n",
    "\n",
    "state_dict = torch.load('checkpoints/clipseg_pascub_ft.pt')\n",
    "model = CLIPSeg(\n",
    "    part_texts=part_texts,\n",
    "    concepts_dict=concept_dict,\n",
    "    meta_category_text='bird',\n",
    "    ft_layers=['d', 'va'],\n",
    "    state_dict=state_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bird head',\n",
       " 'bird beak',\n",
       " 'bird tail',\n",
       " 'bird wing',\n",
       " 'bird leg',\n",
       " 'bird eye',\n",
       " 'bird torso']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.part_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bird head': tensor([[ 0.1576,  0.1256, -0.0866,  ..., -0.3160, -0.1718, -0.0576],\n",
       "         [-0.1047,  0.0813, -0.2178,  ...,  0.0657, -0.1565,  0.0173],\n",
       "         [ 0.1395, -0.2119, -0.0750,  ...,  0.1409,  0.0794,  0.0063],\n",
       "         ...,\n",
       "         [-0.3731, -0.0802, -0.3408,  ...,  0.0267, -0.3798,  0.4257],\n",
       "         [ 0.2522,  0.2424, -0.2402,  ...,  0.0371, -0.0869,  0.2086],\n",
       "         [-0.0781,  0.0294,  0.0346,  ...,  0.0630, -0.0071,  0.0199]],\n",
       "        device='cuda:0'),\n",
       " 'bird beak': tensor([[ 0.2434, -0.0680,  0.1827,  ..., -0.2545, -0.0431,  0.0671],\n",
       "         [-0.0032, -0.3645,  0.0895,  ...,  0.2297, -0.1688, -0.2071],\n",
       "         [ 0.4853, -0.2493,  0.2674,  ..., -0.0249,  0.0837,  0.2307],\n",
       "         ...,\n",
       "         [ 0.1037,  0.2100,  0.1047,  ..., -0.3292,  0.2128,  0.5063],\n",
       "         [ 0.2971, -0.3803, -0.0804,  ...,  0.0131,  0.0424,  0.3985],\n",
       "         [-0.0781,  0.0294,  0.0346,  ...,  0.0630, -0.0071,  0.0199]],\n",
       "        device='cuda:0'),\n",
       " 'bird tail': tensor([[ 0.0362, -0.1581, -0.2146,  ...,  0.0693,  0.0760,  0.0457],\n",
       "         [ 0.0772, -0.2636,  0.3979,  ...,  0.1803, -0.0178,  0.1631],\n",
       "         [-0.0746, -0.3256, -0.2643,  ..., -0.0537,  0.2001,  0.7433],\n",
       "         ...,\n",
       "         [ 0.1954, -0.1479,  0.1818,  ...,  0.1656, -0.1524,  0.0690],\n",
       "         [ 0.1904, -0.3884,  0.2906,  ..., -0.0518, -0.2184,  0.1362],\n",
       "         [-0.0781,  0.0294,  0.0346,  ...,  0.0630, -0.0071,  0.0199]],\n",
       "        device='cuda:0'),\n",
       " 'bird wing': tensor([[ 0.2058, -0.1554, -0.0254,  ..., -0.1016, -0.2452,  0.3712],\n",
       "         [-0.1323,  0.0094,  0.4120,  ...,  0.1170, -0.4367, -0.2286],\n",
       "         [-0.0203,  0.0762,  0.0338,  ...,  0.0817, -0.5622,  0.1393],\n",
       "         ...,\n",
       "         [-0.3276, -0.2749,  0.0594,  ..., -0.0784,  0.1845,  0.1565],\n",
       "         [-0.2147, -0.4594,  0.2409,  ...,  0.1957, -0.0693, -0.1035],\n",
       "         [-0.0781,  0.0294,  0.0346,  ...,  0.0630, -0.0071,  0.0199]],\n",
       "        device='cuda:0'),\n",
       " 'bird leg': tensor([[ 0.2547, -0.1836,  0.1503,  ...,  0.1543,  0.2773, -0.0820],\n",
       "         [ 0.0937, -0.2172, -0.0669,  ..., -0.0284, -0.1607,  0.2012],\n",
       "         [ 0.1008, -0.2220,  0.0477,  ..., -0.0101, -0.1921,  0.3168],\n",
       "         ...,\n",
       "         [ 0.0884, -0.0542, -0.1822,  ...,  0.0425, -0.2575,  0.0717],\n",
       "         [ 0.1240, -0.2608, -0.0711,  ..., -0.0967, -0.0705,  0.1014],\n",
       "         [-0.0781,  0.0294,  0.0346,  ...,  0.0630, -0.0071,  0.0199]],\n",
       "        device='cuda:0'),\n",
       " 'bird eye': tensor([[ 0.2300, -0.0617,  0.4372,  ...,  0.2164, -0.2135,  0.0175],\n",
       "         [ 0.2583,  0.0995,  0.4480,  ..., -0.2626, -0.2275,  0.3088],\n",
       "         [ 0.2367, -0.1785,  0.2585,  ..., -0.1178,  0.0273,  0.2119],\n",
       "         ...,\n",
       "         [ 0.1449, -0.1491,  0.1598,  ..., -0.0387, -0.2539,  0.1672],\n",
       "         [-0.0591, -0.1141,  0.1207,  ...,  0.4803, -0.0345, -0.0789],\n",
       "         [-0.0781,  0.0294,  0.0346,  ...,  0.0630, -0.0071,  0.0199]],\n",
       "        device='cuda:0'),\n",
       " 'bird torso': tensor([[ 0.0422,  0.0208, -0.0272,  ..., -0.0131, -0.2302, -0.1709],\n",
       "         [ 0.1395, -0.2119, -0.0750,  ...,  0.1409,  0.0794,  0.0063],\n",
       "         [-0.0827,  0.2732, -0.0133,  ...,  0.1866, -0.0842,  0.2685],\n",
       "         ...,\n",
       "         [ 0.0375,  0.0096, -0.0449,  ...,  0.2927, -0.0316,  0.2986],\n",
       "         [ 0.1306, -0.1611, -0.4062,  ...,  0.6818, -0.3928,  0.1615],\n",
       "         [-0.0781,  0.0294,  0.0346,  ...,  0.0630, -0.0071,  0.0199]],\n",
       "        device='cuda:0')}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.concept_embedding_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from data.cub_dataset_v2 import CUBDatasetSimple\n",
    "\n",
    "def collate_fn(batch):\n",
    "    image_list, label_list = list(zip(*batch))\n",
    "    return image_list, torch.stack(label_list)\n",
    "\n",
    "dataset_train = CUBDatasetSimple(os.path.join('datasets', 'CUB'), split='train')\n",
    "dataloader_train = DataLoader(dataset=dataset_train, collate_fn=collate_fn, batch_size=2, shuffle=True)\n",
    "dataloader_train_iter = iter(dataloader_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    image_list, targets = next(dataloader_train_iter)\n",
    "    loss, logits = model(image_list, targets)\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import defaultdict\n",
    "\n",
    "def load_concepts():\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    def attach_part_name(concepts: list[str], part_name: str):\n",
    "        concepts_processed = []\n",
    "        for cpt in concepts:\n",
    "            doc = nlp(cpt)\n",
    "            if not any('NOUN' == word.pos_ for word in doc):\n",
    "                cpt = cpt + ' ' + part_name\n",
    "            # if 'than' in cpt or 'male' in cpt:  # Note that this would cause Purple Finch to have 0 concept for torso and American GoldFinch to have 0 concept for head\n",
    "            #     continue \n",
    "            concepts_processed.append(cpt)\n",
    "        return concepts_processed\n",
    "\n",
    "    concept_sets = defaultdict(set)\n",
    "    with open('concepts/CUB/concepts_processed.json', 'rb') as fp:\n",
    "        concepts_processed = json.load(fp=fp)\n",
    "\n",
    "    # Add a noun to purely adjective concepts\n",
    "    for class_name, concept_dict in concepts_processed.items():\n",
    "        for part_name, concepts in concept_dict.items():\n",
    "            concepts_with_part_name = attach_part_name(concepts, part_name)\n",
    "            concept_dict[part_name] = concepts_with_part_name\n",
    "            concept_sets[part_name].update(concepts_with_part_name)\n",
    "\n",
    "    concept_sets_sorted = {k: sorted(list(v)) for k, v in concept_sets.items()}\n",
    "    return concept_sets_sorted\n",
    "\n",
    "concepts_dict = load_concepts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_concepts = set()\n",
    "for v in concepts_dict.values():\n",
    "    all_concepts.update(v)\n",
    "\n",
    "all_concepts = list(all_concepts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.tokenization_utils_base import BatchEncoding\n",
    "\n",
    "with torch.no_grad():\n",
    "    concepts_token = model.clipseg_processor.tokenizer(all_concepts, return_tensors='pt', padding='max_length')\n",
    "    embeddings = model.clipseg_model.get_conditional_embeddings(**concepts_token.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(concepts_token.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concepts_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import linear_sum_assignment\n",
    "\n",
    "prototypes = torch.rand(7, 512, 50)\n",
    "database = torch.randn(900, 512)\n",
    "prototypes = prototypes.permute(0, 2, 1).reshape(7 * 50, 512)\n",
    "affinities = pairwise_cosine_similarity(prototypes, database).numpy()\n",
    "\n",
    "rows, cols = linear_sum_assignment(affinities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows.shape, cols.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.functional.pairwise import pairwise_cosine_similarity\n",
    "weights = torch.ones(7, 512, 100)\n",
    "encodings = torch.randn(1200, 512)\n",
    "\n",
    "weights_flatten = weights.permute(0, 2, 1).reshape(7 * 100, 512)\n",
    "\n",
    "sims =pairwise_cosine_similarity(weights_flatten, encodings)\n",
    "1 - torch.mean(sims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.part_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mahalanobis Distance Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "clip_model, clip_preprocess = clip.load('RN50')\n",
    "\n",
    "with open('concepts/CUB/concepts_processed.json', 'r') as fp:\n",
    "    concept_dict = json.load(fp=fp)\n",
    "\n",
    "unique_concepts = set()\n",
    "for class_name, class_concepts in concept_dict.items():\n",
    "    for concepts in class_concepts.values():\n",
    "        unique_concepts.update(concepts)\n",
    "\n",
    "unique_concepts = sorted(list(unique_concepts))\n",
    "\n",
    "concepts_tokenized = clip.tokenize(unique_concepts).to(device)\n",
    "with torch.no_grad():\n",
    "    concepts_encoded = clip_model.encode_text(concepts_tokenized)\n",
    "\n",
    "concepts_encoded = concepts_encoded.to(torch.float32)\n",
    "concepts_encoded_norm = F.normalize(concepts_encoded, dim=-1)\n",
    "concepts_mean = torch.mean(concepts_encoded_norm, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.covariance import EmpiricalCovariance\n",
    "\n",
    "cov = EmpiricalCovariance().fit(concepts_encoded.cpu().numpy())\n",
    "_sigma_inv = cov.get_precision()\n",
    "sigma_inv = torch.from_numpy(_sigma_inv).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mahalanobis_v1(samples, mu, sigma_inv):\n",
    "    dists = torch.sum((samples - mu) @ sigma_inv * (samples - mu))\n",
    "    return dists\n",
    "\n",
    "def mahalanobis_v2(samples, mu, sigma_inv):\n",
    "    dists = []\n",
    "    for s in samples: \n",
    "        d = (s - mu) @ sigma_inv @ (s - mu)\n",
    "        dists.append(d)\n",
    "    return sum(dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.randn(10, 1024).to('cuda')\n",
    "sample2 = concepts_encoded[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah1 = mahalanobis_v1(sample2.to(torch.float64), concepts_mean.to(torch.float64), sigma_inv)\n",
    "mah2 = mahalanobis_v2(sample2.to(torch.float64), concepts_mean.to(torch.float64), sigma_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah1, mah2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah1 = mahalanobis_v1(sample1.to(torch.float64), concepts_mean.to(torch.float64), sigma_inv)\n",
    "mah2 = mahalanobis_v2(sample1.to(torch.float64), concepts_mean.to(torch.float64), sigma_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mah1, mah2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHBLoss(nn.Module):\n",
    "    def __init__(self, T: torch.Tensor, coef=1e-4) -> None:\n",
    "        super().__init__()\n",
    "        self.coef = coef\n",
    "        \n",
    "        self.T_norm = F.normalize(T, dim=-1)\n",
    "        self.T_mu = torch.mean(self.T_norm, dim=0)\n",
    "        self.T_sigma = torch.cov(self.T_norm.T)\n",
    "\n",
    "        self.T_sigma_inv = torch.inverse(self.T_norm)\n",
    "    \n",
    "    def forward(self, samples):\n",
    "        assert samples.dim in [2, 3]\n",
    "        if samples.dim == 3:\n",
    "            n, m, d = samples.shape\n",
    "            samples_flat = samples.view(n*m, d)\n",
    "        \n",
    "        return self.coef * (samples_flat - self.T_mu) @ sigma_inv @ (samples_flat - concepts_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "research",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
